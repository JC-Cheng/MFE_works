{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFE230T\n",
    "\n",
    "## Jen-Chieh Cheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting signals from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 330\n"
     ]
    }
   ],
   "source": [
    "tweets_file = open(\"./tweets/1502820001-tweets.txt\", 'rb')\n",
    "lines = tweets_file.readlines()\n",
    "print \"Number of tweets: %d\" % len(lines)\n",
    "tweets_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'contributors': None,\n",
      " u'coordinates': None,\n",
      " u'created_at': u'Tue Aug 15 17:58:26 +0000 2017',\n",
      " u'entities': {u'hashtags': [{u'indices': [45, 52], u'text': u'patent'},\n",
      "                             {u'indices': [65, 68], u'text': u'IP'}],\n",
      "               u'symbols': [{u'indices': [103, 108], u'text': u'GOOG'},\n",
      "                            {u'indices': [109, 112], u'text': u'FB'}],\n",
      "               u'urls': [{u'display_url': u'iam-media.com/blog/Detail.as\\u2026',\n",
      "                          u'expanded_url': u'http://www.iam-media.com/blog/Detail.aspx?g=afc6cc58-706a-475d-906a-fd85bd1e49f1',\n",
      "                          u'indices': [113, 136],\n",
      "                          u'url': u'https://t.co/FiHWRiETq3'}],\n",
      "               u'user_mentions': [{u'id': 108564136,\n",
      "                                   u'id_str': u'108564136',\n",
      "                                   u'indices': [3, 16],\n",
      "                                   u'name': u'IAM',\n",
      "                                   u'screen_name': u'IAM_magazine'}]},\n",
      " u'favorite_count': 0,\n",
      " u'favorited': False,\n",
      " u'geo': None,\n",
      " u'id': 897517856702812160,\n",
      " u'id_str': u'897517856702812160',\n",
      " u'in_reply_to_screen_name': None,\n",
      " u'in_reply_to_status_id': None,\n",
      " u'in_reply_to_status_id_str': None,\n",
      " u'in_reply_to_user_id': None,\n",
      " u'in_reply_to_user_id_str': None,\n",
      " u'is_quote_status': False,\n",
      " u'lang': u'en',\n",
      " u'metadata': {u'iso_language_code': u'en', u'result_type': u'recent'},\n",
      " u'place': None,\n",
      " u'possibly_sensitive': False,\n",
      " u'retweet_count': 9,\n",
      " u'retweeted': False,\n",
      " u'retweeted_status': {u'contributors': None,\n",
      "                       u'coordinates': None,\n",
      "                       u'created_at': u'Mon Aug 14 18:37:19 +0000 2017',\n",
      "                       u'entities': {u'hashtags': [{u'indices': [27, 34],\n",
      "                                                    u'text': u'patent'},\n",
      "                                                   {u'indices': [47, 50],\n",
      "                                                    u'text': u'IP'}],\n",
      "                                     u'symbols': [{u'indices': [85, 90],\n",
      "                                                   u'text': u'GOOG'},\n",
      "                                                  {u'indices': [91, 94],\n",
      "                                                   u'text': u'FB'}],\n",
      "                                     u'urls': [{u'display_url': u'iam-media.com/blog/Detail.as\\u2026',\n",
      "                                                u'expanded_url': u'http://www.iam-media.com/blog/Detail.aspx?g=afc6cc58-706a-475d-906a-fd85bd1e49f1',\n",
      "                                                u'indices': [95, 118],\n",
      "                                                u'url': u'https://t.co/FiHWRiETq3'}],\n",
      "                                     u'user_mentions': []},\n",
      "                       u'favorite_count': 4,\n",
      "                       u'favorited': False,\n",
      "                       u'geo': None,\n",
      "                       u'id': 897165251254382593,\n",
      "                       u'id_str': u'897165251254382593',\n",
      "                       u'in_reply_to_screen_name': None,\n",
      "                       u'in_reply_to_status_id': None,\n",
      "                       u'in_reply_to_status_id_str': None,\n",
      "                       u'in_reply_to_user_id': None,\n",
      "                       u'in_reply_to_user_id_str': None,\n",
      "                       u'is_quote_status': False,\n",
      "                       u'lang': u'en',\n",
      "                       u'metadata': {u'iso_language_code': u'en',\n",
      "                                     u'result_type': u'recent'},\n",
      "                       u'place': None,\n",
      "                       u'possibly_sensitive': False,\n",
      "                       u'retweet_count': 9,\n",
      "                       u'retweeted': False,\n",
      "                       u'source': u'<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
      "                       u'text': u'Exclusive: In major Valley #patent move Google #IP head Allen Lo is joining Facebook $GOOG $FB https://t.co/FiHWRiETq3',\n",
      "                       u'truncated': False,\n",
      "                       u'user': {u'contributors_enabled': False,\n",
      "                                 u'created_at': u'Tue Jan 26 09:55:36 +0000 2010',\n",
      "                                 u'default_profile': False,\n",
      "                                 u'default_profile_image': False,\n",
      "                                 u'description': u'Please note: Retweets should not be regarded as endorsements.',\n",
      "                                 u'entities': {u'description': {u'urls': []},\n",
      "                                               u'url': {u'urls': [{u'display_url': u'iam-media.com',\n",
      "                                                                   u'expanded_url': u'http://www.iam-media.com',\n",
      "                                                                   u'indices': [0,\n",
      "                                                                                22],\n",
      "                                                                   u'url': u'http://t.co/lSHJSRwQa1'}]}},\n",
      "                                 u'favourites_count': 128,\n",
      "                                 u'follow_request_sent': False,\n",
      "                                 u'followers_count': 5320,\n",
      "                                 u'following': False,\n",
      "                                 u'friends_count': 269,\n",
      "                                 u'geo_enabled': True,\n",
      "                                 u'has_extended_profile': False,\n",
      "                                 u'id': 108564136,\n",
      "                                 u'id_str': u'108564136',\n",
      "                                 u'is_translation_enabled': False,\n",
      "                                 u'is_translator': False,\n",
      "                                 u'lang': u'en',\n",
      "                                 u'listed_count': 210,\n",
      "                                 u'location': u'London-Hong Kong-Washington DC',\n",
      "                                 u'name': u'IAM',\n",
      "                                 u'notifications': False,\n",
      "                                 u'profile_background_color': u'DFDFDF',\n",
      "                                 u'profile_background_image_url': u'http://pbs.twimg.com/profile_background_images/536860588987514881/whGihSNG.png',\n",
      "                                 u'profile_background_image_url_https': u'https://pbs.twimg.com/profile_background_images/536860588987514881/whGihSNG.png',\n",
      "                                 u'profile_background_tile': False,\n",
      "                                 u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/108564136/1416832322',\n",
      "                                 u'profile_image_url': u'http://pbs.twimg.com/profile_images/536860397869887488/ddZRusGP_normal.png',\n",
      "                                 u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/536860397869887488/ddZRusGP_normal.png',\n",
      "                                 u'profile_link_color': u'F11B23',\n",
      "                                 u'profile_sidebar_border_color': u'FFFFFF',\n",
      "                                 u'profile_sidebar_fill_color': u'DDEEF6',\n",
      "                                 u'profile_text_color': u'333333',\n",
      "                                 u'profile_use_background_image': True,\n",
      "                                 u'protected': False,\n",
      "                                 u'screen_name': u'IAM_magazine',\n",
      "                                 u'statuses_count': 15380,\n",
      "                                 u'time_zone': u'London',\n",
      "                                 u'translator_type': u'none',\n",
      "                                 u'url': u'http://t.co/lSHJSRwQa1',\n",
      "                                 u'utc_offset': 3600,\n",
      "                                 u'verified': False}},\n",
      " u'source': u'<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
      " u'text': u'RT @IAM_magazine: Exclusive: In major Valley #patent move Google #IP head Allen Lo is joining Facebook $GOOG $FB https://t.co/FiHWRiETq3',\n",
      " u'truncated': False,\n",
      " u'user': {u'contributors_enabled': False,\n",
      "           u'created_at': u'Wed May 02 22:00:01 +0000 2007',\n",
      "           u'default_profile': True,\n",
      "           u'default_profile_image': False,\n",
      "           u'description': u'Developer, patent lawyer, typo checker. Tweets tend to be about Apple, IP, faith, truth, beauty, mundanity. I work for @parallel_tw.',\n",
      "           u'entities': {u'description': {u'urls': []}},\n",
      "           u'favourites_count': 8369,\n",
      "           u'follow_request_sent': False,\n",
      "           u'followers_count': 1257,\n",
      "           u'following': False,\n",
      "           u'friends_count': 3520,\n",
      "           u'geo_enabled': True,\n",
      "           u'has_extended_profile': False,\n",
      "           u'id': 5725012,\n",
      "           u'id_str': u'5725012',\n",
      "           u'is_translation_enabled': False,\n",
      "           u'is_translator': False,\n",
      "           u'lang': u'en',\n",
      "           u'listed_count': 101,\n",
      "           u'location': u'Boston, MA',\n",
      "           u'name': u'Michael Saji',\n",
      "           u'notifications': False,\n",
      "           u'profile_background_color': u'C0DEED',\n",
      "           u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "           u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "           u'profile_background_tile': False,\n",
      "           u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/5725012/1394588981',\n",
      "           u'profile_image_url': u'http://pbs.twimg.com/profile_images/613751130439286784/neIdTi5y_normal.png',\n",
      "           u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/613751130439286784/neIdTi5y_normal.png',\n",
      "           u'profile_link_color': u'1DA1F2',\n",
      "           u'profile_sidebar_border_color': u'C0DEED',\n",
      "           u'profile_sidebar_fill_color': u'DDEEF6',\n",
      "           u'profile_text_color': u'333333',\n",
      "           u'profile_use_background_image': True,\n",
      "           u'protected': False,\n",
      "           u'screen_name': u'saji',\n",
      "           u'statuses_count': 12722,\n",
      "           u'time_zone': u'Eastern Time (US & Canada)',\n",
      "           u'translator_type': u'none',\n",
      "           u'url': None,\n",
      "           u'utc_offset': -14400,\n",
      "           u'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(json.loads(lines[0].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1502820001-tweets.txt\n",
      "1502906402-tweets.txt\n",
      "1502992801-tweets.txt\n",
      "1503079201-tweets.txt\n",
      "1503165601-tweets.txt\n",
      "1503252001-tweets.txt\n",
      "1503338401-tweets.txt\n",
      "1503424801-tweets.txt\n",
      "1503511201-tweets.txt\n",
      "1503597601-tweets.txt\n",
      "1503684001-tweets.txt\n",
      "1503770401-tweets.txt\n",
      "1503856801-tweets.txt\n",
      "1503943201-tweets.txt\n",
      "1504029601-tweets.txt\n",
      "1504116001-tweets.txt\n",
      "1504202401-tweets.txt\n",
      "1504288801-tweets.txt\n",
      "1504375201-tweets.txt\n",
      "1504461601-tweets.txt\n",
      "1504548001-tweets.txt\n",
      "1504634401-tweets.txt\n",
      "1504720801-tweets.txt\n",
      "1504807201-tweets.txt\n",
      "1504893601-tweets.txt\n",
      "1504980001-tweets.txt\n",
      "1505066401-tweets.txt\n",
      "1505152801-tweets.txt\n",
      "1505239201-tweets.txt\n",
      "1505325601-tweets.txt\n",
      "1505412001-tweets.txt\n",
      "1505498401-tweets.txt\n",
      "1505584801-tweets.txt\n",
      "1505671201-tweets.txt\n",
      "1505757601-tweets.txt\n",
      "1505844001-tweets.txt\n",
      "1505930401-tweets.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15513"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all tweets files\n",
    "\n",
    "lines = list()\n",
    "\n",
    "for f in os.listdir('tweets'):\n",
    "    print f\n",
    "    tweet_file = open('tweets/' + f, 'rb')\n",
    "    lines += tweet_file.readlines()\n",
    "    tweet_file.close()\n",
    "    \n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO use all twitter files\n",
    "data = defaultdict(dict)\n",
    "i=0\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    tweet = json.loads(line.strip())\n",
    "    if 'text' in tweet: # only messages contains 'text' field is a tweet\n",
    "        ts = time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "        data[i][\"time\"] = time.mktime(ts)  \n",
    "        data[i][\"text\"] = tweet['text']\n",
    "    if 'urls' in tweet['entities']:\n",
    "        #print tweet['entities']['urls']\n",
    "        data[i][\"urls\"] = len(tweet['entities']['urls'])\n",
    "    if 'media' in tweet['entities']:\n",
    "        #print tweet['entities']['media']\n",
    "        data[i][\"media\"] = len(tweet['entities']['media'])\n",
    "    if 'hashtags' in tweet['entities']:\n",
    "        data[i][\"hashtags\"] = len(tweet['entities']['hashtags'])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': u'RT @IAM_magazine: Exclusive: In major Valley #patent move Google #IP head Allen Lo is joining Facebook $GOOG $FB https://t.co/FiHWRiETq3', 'hashtags': 2, 'urls': 1, 'time': 1502845106.0}\n",
      "{'text': u'RT @arnabch01: #investors massive bubble in #tech be careful $AAPL $GOOG $MSFT $AMZN $FB $NFLX $TSLA $CSCO $INTC $NVDA $ZNGA $ORCL $JD $MU\\u2026', 'hashtags': 2, 'urls': 0, 'time': 1502844958.0}\n"
     ]
    }
   ],
   "source": [
    "### which other signals could be useful? \n",
    "print data[0]\n",
    "print data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#working with text\n",
    "#tokenizer for tweets\n",
    "tknzr = TweetTokenizer(strip_handles=True) #(strip_handles=True, reduce_len=True)\n",
    "\n",
    "for i, info in data.items():  \n",
    "    text = info['text'].lower().encode('utf-8').decode('ascii','ignore') # content of the tweet\n",
    "    #print text\n",
    "    text = re.sub(r\"http\\S*\", '', text) #remove urls\n",
    "    text = re.sub(r\"^rt\", '', text) #remove rt\n",
    "    #print text\n",
    "    words = tknzr.tokenize(text)\n",
    "    #print words\n",
    "    data[i]['exclamations'] = words.count('!')\n",
    "    data[i]['questions'] = words.count('?')\n",
    "    data[i]['dollar'] = words.count('$')\n",
    "    data[i]['text'] = \" \".join(words).encode('utf-8')\n",
    "    data[i]['text'] = re.sub(r'[^\\w\\s]', '', data[i]['text'])\n",
    "    data[i]['text'] = re.sub(r'\\d+', '', data[i]['text'])\n",
    "    data[i]['num_words'] = len(text) \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'urls': 1, 'text': ' exclusive  in major valley patent move google ip head allen lo is joining facebook  goog  fb', 'hashtags': 2, 'dollar': 2, 'questions': 0, 'time': 1502845106.0, 'exclamations': 0, 'num_words': 111}\n",
      "{'urls': 0, 'text': ' investors massive bubble in tech be careful  aapl  goog  msft  amzn  fb  nflx  tsla  csco  intc  nvda  znga  orcl  jd  mu', 'hashtags': 2, 'dollar': 14, 'questions': 0, 'time': 1502844958.0, 'exclamations': 0, 'num_words': 136}\n"
     ]
    }
   ],
   "source": [
    "print data[0]\n",
    "print data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>dollar</th>\n",
       "      <th>questions</th>\n",
       "      <th>time</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>num_words</th>\n",
       "      <th>media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15513.000000</td>\n",
       "      <td>15513.000000</td>\n",
       "      <td>15513.000000</td>\n",
       "      <td>15513.000000</td>\n",
       "      <td>1.551300e+04</td>\n",
       "      <td>15513.000000</td>\n",
       "      <td>15513.000000</td>\n",
       "      <td>1153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.684200</td>\n",
       "      <td>0.853671</td>\n",
       "      <td>3.888094</td>\n",
       "      <td>0.096629</td>\n",
       "      <td>1.504373e+09</td>\n",
       "      <td>0.115516</td>\n",
       "      <td>95.372075</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.536803</td>\n",
       "      <td>1.932290</td>\n",
       "      <td>4.349792</td>\n",
       "      <td>0.334164</td>\n",
       "      <td>9.147151e+05</td>\n",
       "      <td>0.450559</td>\n",
       "      <td>30.699871</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.502505e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.503607e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.504389e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.505175e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.505956e+09</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               urls      hashtags        dollar     questions          time  \\\n",
       "count  15513.000000  15513.000000  15513.000000  15513.000000  1.551300e+04   \n",
       "mean       0.684200      0.853671      3.888094      0.096629  1.504373e+09   \n",
       "std        0.536803      1.932290      4.349792      0.334164  9.147151e+05   \n",
       "min        0.000000      0.000000      0.000000      0.000000  1.502505e+09   \n",
       "25%        0.000000      0.000000      1.000000      0.000000  1.503607e+09   \n",
       "50%        1.000000      0.000000      2.000000      0.000000  1.504389e+09   \n",
       "75%        1.000000      1.000000      5.000000      0.000000  1.505175e+09   \n",
       "max        3.000000     12.000000     24.000000      6.000000  1.505956e+09   \n",
       "\n",
       "       exclamations     num_words   media  \n",
       "count  15513.000000  15513.000000  1153.0  \n",
       "mean       0.115516     95.372075     1.0  \n",
       "std        0.450559     30.699871     0.0  \n",
       "min        0.000000      6.000000     1.0  \n",
       "25%        0.000000     72.000000     1.0  \n",
       "50%        0.000000     99.000000     1.0  \n",
       "75%        0.000000    117.000000     1.0  \n",
       "max        7.000000    152.000000     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>dollar</th>\n",
       "      <th>questions</th>\n",
       "      <th>time</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>num_words</th>\n",
       "      <th>media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>exclusive  in major valley patent move google...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>investors massive bubble in tech be careful  ...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>google  goog is the embodiment of modern pc f...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>timberr    iwm  spy  tlt  gs  gld  btc  goog ...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bank of nova scotia buys  shares of alphabet i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   urls                                               text  hashtags  dollar  \\\n",
       "0     1   exclusive  in major valley patent move google...         2       2   \n",
       "1     0   investors massive bubble in tech be careful  ...         2      14   \n",
       "2     0   google  goog is the embodiment of modern pc f...         6       1   \n",
       "3     0   timberr    iwm  spy  tlt  gs  gld  btc  goog ...         0      21   \n",
       "4     1  bank of nova scotia buys  shares of alphabet i...         0       1   \n",
       "\n",
       "   questions          time  exclamations  num_words  media  \n",
       "0          0  1.502845e+09             0        111    NaN  \n",
       "1          0  1.502845e+09             0        136    NaN  \n",
       "2          0  1.502845e+09             0        126    NaN  \n",
       "3          0  1.502845e+09             2        137    NaN  \n",
       "4          0  1.502845e+09             0         62    NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8434, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates('text')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amaazing jjaayy chou'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shrink_word(w):\n",
    "    \n",
    "    if len(w) < 3:\n",
    "        return w\n",
    "    else:\n",
    "        w = list(w)\n",
    "        i = 2\n",
    "        while i < len(w):\n",
    "            if w[i-1] == w[i-2] and w[i-1] == w[i]:\n",
    "                w.pop(i)\n",
    "            else:\n",
    "                i+=1\n",
    "        return ''.join(w)\n",
    "    \n",
    "shrink_word('amaaaaaazing jjjjjjaayy chou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOWER_SPACE = string.lowercase + ' '\n",
    "\n",
    "def text_parser(text):\n",
    "    \n",
    "    cleaned = text.lower().replace('\\n',' ').replace('\\t', ' ')\n",
    "    \n",
    "    for char in string.punctuation:\n",
    "        cleaned = cleaned.replace(char,' ')\n",
    "    cleaned = filter(lambda char: char in LOWER_SPACE, cleaned)\n",
    "    \n",
    "    cleaned = ' '.join(map(shrink_word, cleaned.split()))\n",
    "    \n",
    "    return str(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-11390a40f579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jenchieh/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jenchieh/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2169\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3557)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3240)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8564)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8508)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 30"
     ]
    }
   ],
   "source": [
    "text_parser(df.text[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(text_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          frequency\n",
      "count  11047.000000\n",
      "mean       7.113334\n",
      "std       42.310521\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        2.000000\n",
      "75%        4.000000\n",
      "max     1632.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aab</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaba</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aac</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aae</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aal</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aanndd</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaoi</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aap</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapl</th>\n",
       "      <td>1632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequency\n",
       "aa             23\n",
       "aab             2\n",
       "aaba            5\n",
       "aac             1\n",
       "aae             1\n",
       "aal             8\n",
       "aanndd          1\n",
       "aaoi            7\n",
       "aap            10\n",
       "aapl         1632"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word_vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
    "sparse_matrix = word_vectorizer.fit_transform(df['text'])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "words = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "print words.describe()\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         frequency\n",
      "count  2480.000000\n",
      "mean     25.948387\n",
      "std      86.694647\n",
      "min       5.000000\n",
      "25%       6.000000\n",
      "50%      10.000000\n",
      "75%      20.000000\n",
      "max    1632.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaba</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aal</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaoi</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aap</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapl</th>\n",
       "      <td>1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapls</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbv</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abeo</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequency\n",
       "aa            23\n",
       "aaba           5\n",
       "aal            8\n",
       "aaoi           7\n",
       "aap           10\n",
       "aapl        1632\n",
       "aapls         11\n",
       "ab             6\n",
       "abbv          11\n",
       "abeo           5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = CountVectorizer(analyzer='word', stop_words='english',min_df=5, max_df=3000)\n",
    "sparse_matrix = word_vectorizer.fit_transform(df['text'])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "words = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "print words.describe()\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding structure in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del words\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_features = 250\n",
    "n_components = 20\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "### Counts\n",
    "tf_vectorizer = CountVectorizer(min_df=5, max_df=3000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df.text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "#print tf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5,max_df=3000,stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.text)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding topics with LDA\n",
    "\n",
    "**Remark**:\n",
    "As we exploit all tweet files, the both LDA models (TF and TFIDF) generate more diversified topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: long data way days aapl future volume technology did run\n",
      "Topic #1: corporation shares business management position alphabet international ibm llc stake\n",
      "Topic #2: tsla options gm hold better cars break investors bond half\n",
      "Topic #3: stocks trading fb check tech twtr video nice alerts watch\n",
      "Topic #4: amzn trade amazon wmt good car th tgt think really\n",
      "Topic #5: snap time snapchat rating looking value soon does coming china\n",
      "Topic #6: im review musk past study build program user talk ios\n",
      "Topic #7: aapl fb spy tsla amzn nflx baba stocks nvda qq\n",
      "Topic #8: tsla aapl tesla apple like calls year model apples profits\n",
      "Topic #9: msft twtr crm ibm aapl brk agn amzn vrx iep\n",
      "Topic #10: aapl short apple chart look jnj ceo september tv youre\n",
      "Topic #11: box cloud sold holding buys ipo acquires home moving entry\n",
      "Topic #12: msft microsoft new today blockchain ready high highs relevant event\n",
      "Topic #13: big holdings news daily lol azfl profit huge drys blue\n",
      "Topic #14: earnings vs intc corp analysis csco mu report update bidu\n",
      "Topic #15: buy sell dont buying company stock right weekly low ahead\n",
      "Topic #16: ibm know sep puts watson dividend say breaking things worth\n",
      "Topic #17: amzn week msft ai amazon high goog billion foods wfm\n",
      "Topic #18: just day read price best going getting bitcoin ive trades\n",
      "Topic #19: fb goog stock googl aapl market facebook iphone amzn free\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, learning_method='online')\n",
    "lda.fit(tf)\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: tsla spy long nflx qq aapl car amzn fb tgt\n",
      "Topic #1: business ibm international machines corporation bought sold shares bank amzn\n",
      "Topic #2: snap getting rating buy great box chart ive trades day\n",
      "Topic #3: holdings ibm tsla crm brk msft amzn model tesla agn\n",
      "Topic #4: fb amzn googl goog snap time data earnings year billion\n",
      "Topic #5: look think vs china highs finally blue ibm analysis better\n",
      "Topic #6: like price just tsla days share hit make stock watson\n",
      "Topic #7: good going trade support holding really snap company puts recommend\n",
      "Topic #8: microsoft alphabet msft management shares position llc goog stake capital\n",
      "Topic #9: aapl stocks investing stockmarket watch nvda amzn intc goog amd\n",
      "Topic #10: relevant invest life premium azure launches development acquires ahgif chk\n",
      "Topic #11: trading fb tsla twtr snap aapl money stocks free bitcoin\n",
      "Topic #12: break play key action news musk drys update discovering monday\n",
      "Topic #13: new best companies dividend high aapl moving does lnkd articles\n",
      "Topic #14: options largest th coming amazing week box open event cuts\n",
      "Topic #15: ibm read ai way says did say trend amzn thanks\n",
      "Topic #16: snapchat snap google tech tv goog heres twitter content help\n",
      "Topic #17: years low revenue boosted bby months level solar machs box\n",
      "Topic #18: iphone aapl apple short gm close looks snap review half\n",
      "Topic #19: snap calls looking buying sep right thats bad dont aprn\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, learning_method='online')\n",
    "lda.fit(tfidf)\n",
    "print_top_words(lda, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive = pd.read_csv('data/positive-words.txt', names=['a'])\n",
    "positive = set(positive['a'].tolist())\n",
    "\n",
    "negative = pd.read_csv('data/negative-words.txt', names=['a'])\n",
    "negative = set(negative['a'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_positive = []\n",
    "count_negative = []\n",
    "for i, row in df.iterrows():\n",
    "    commonp = set(row['text'].split()).intersection(positive) \n",
    "    count_positive.append(len(commonp))\n",
    "    commonn = set(row['text'].split()).intersection(negative) \n",
    "    count_negative.append(len(commonn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>dollar</th>\n",
       "      <th>questions</th>\n",
       "      <th>time</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>num_words</th>\n",
       "      <th>media</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>exclusive in major valley patent move google i...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>investors massive bubble in tech be careful aa...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>google goog is the embodiment of modern pc fas...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>timberr iwm spy tlt gs gld btc goog aapl fb nf...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bank of nova scotia buys shares of alphabet in...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>alphabet inc goog stake raised by north star a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>the machines keep getting smarter can your por...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>as alphabet goog valuation rose robshaw julian...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>warren averett asset management llc boosts pos...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>goog himx vuzi great article</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    urls                                               text  hashtags  dollar  \\\n",
       "0      1  exclusive in major valley patent move google i...         2       2   \n",
       "1      0  investors massive bubble in tech be careful aa...         2      14   \n",
       "2      0  google goog is the embodiment of modern pc fas...         6       1   \n",
       "3      0  timberr iwm spy tlt gs gld btc goog aapl fb nf...         0      21   \n",
       "4      1  bank of nova scotia buys shares of alphabet in...         0       1   \n",
       "5      1  alphabet inc goog stake raised by north star a...         0       1   \n",
       "6      1  the machines keep getting smarter can your por...         0       2   \n",
       "8      0  as alphabet goog valuation rose robshaw julian...         0       2   \n",
       "9      1  warren averett asset management llc boosts pos...         0       1   \n",
       "10     1                       goog himx vuzi great article         0       3   \n",
       "\n",
       "    questions          time  exclamations  num_words  media  positive  \\\n",
       "0           0  1.502845e+09             0        111    NaN         0   \n",
       "1           0  1.502845e+09             0        136    NaN         0   \n",
       "2           0  1.502845e+09             0        126    NaN         2   \n",
       "3           0  1.502845e+09             2        137    NaN         0   \n",
       "4           0  1.502845e+09             0         62    NaN         0   \n",
       "5           0  1.502845e+09             0         69    NaN         0   \n",
       "6           1  1.502845e+09             0        102    NaN         2   \n",
       "8           0  1.502844e+09             0        105    NaN         0   \n",
       "9           0  1.502844e+09             0         75    NaN         0   \n",
       "10          0  1.502844e+09             0         37    NaN         1   \n",
       "\n",
       "    negative  label  \n",
       "0          0      0  \n",
       "1          0      0  \n",
       "2          1      0  \n",
       "3          0      0  \n",
       "4          0      0  \n",
       "5          0      0  \n",
       "6          0      1  \n",
       "8          0      0  \n",
       "9          0      0  \n",
       "10         0      1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['positive'] = count_positive\n",
    "df['negative'] = count_negative\n",
    "df['label'] = [int(p > 0) - int(n > 0) for p, n in zip(count_positive, count_negative)]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    4969\n",
       " 1    2219\n",
       "-1    1246\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>dollar</th>\n",
       "      <th>questions</th>\n",
       "      <th>time</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>num_words</th>\n",
       "      <th>media</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8434.000000</td>\n",
       "      <td>8434.000000</td>\n",
       "      <td>8434.000000</td>\n",
       "      <td>8434.000000</td>\n",
       "      <td>8.434000e+03</td>\n",
       "      <td>8434.000000</td>\n",
       "      <td>8434.000000</td>\n",
       "      <td>792.0</td>\n",
       "      <td>8434.000000</td>\n",
       "      <td>8434.000000</td>\n",
       "      <td>8434.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.701209</td>\n",
       "      <td>0.521224</td>\n",
       "      <td>2.894475</td>\n",
       "      <td>0.107897</td>\n",
       "      <td>1.504405e+09</td>\n",
       "      <td>0.117501</td>\n",
       "      <td>90.078018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.424354</td>\n",
       "      <td>0.277330</td>\n",
       "      <td>0.115366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.541971</td>\n",
       "      <td>1.298674</td>\n",
       "      <td>3.413585</td>\n",
       "      <td>0.358498</td>\n",
       "      <td>9.296125e+05</td>\n",
       "      <td>0.460790</td>\n",
       "      <td>30.275436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.665263</td>\n",
       "      <td>0.567962</td>\n",
       "      <td>0.630535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.502506e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.503618e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.504391e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.505261e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.505956e+09</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              urls     hashtags       dollar    questions          time  \\\n",
       "count  8434.000000  8434.000000  8434.000000  8434.000000  8.434000e+03   \n",
       "mean      0.701209     0.521224     2.894475     0.107897  1.504405e+09   \n",
       "std       0.541971     1.298674     3.413585     0.358498  9.296125e+05   \n",
       "min       0.000000     0.000000     0.000000     0.000000  1.502506e+09   \n",
       "25%       0.000000     0.000000     1.000000     0.000000  1.503618e+09   \n",
       "50%       1.000000     0.000000     2.000000     0.000000  1.504391e+09   \n",
       "75%       1.000000     0.000000     3.000000     0.000000  1.505261e+09   \n",
       "max       3.000000    12.000000    24.000000     6.000000  1.505956e+09   \n",
       "\n",
       "       exclamations    num_words  media     positive     negative        label  \n",
       "count   8434.000000  8434.000000  792.0  8434.000000  8434.000000  8434.000000  \n",
       "mean       0.117501    90.078018    1.0     0.424354     0.277330     0.115366  \n",
       "std        0.460790    30.275436    0.0     0.665263     0.567962     0.630535  \n",
       "min        0.000000     6.000000    1.0     0.000000     0.000000    -1.000000  \n",
       "25%        0.000000    68.000000    1.0     0.000000     0.000000     0.000000  \n",
       "50%        0.000000    92.000000    1.0     0.000000     0.000000     0.000000  \n",
       "75%        0.000000   114.000000    1.0     1.000000     0.000000     1.000000  \n",
       "max        7.000000   152.000000    1.0     6.000000     4.000000     1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      the machines keep getting smarter can your por...\n",
       "10                          goog himx vuzi great article\n",
       "11     apples bargaining power rising google said to ...\n",
       "18     ai robotics bigdata genomics stemcell advances...\n",
       "19     applewatch to support both lte and nonlte mode...\n",
       "28     beijing transit contactless mpayment system ex...\n",
       "38     tweaktown pr asrockinfo introduces the x iot r...\n",
       "41     pr introduces the x iot router for smart homes...\n",
       "59     since its ipo home depot is actually outperfor...\n",
       "64     amzns same day pick up locations are being cal...\n",
       "68     would be amazed if jana partners manage to sel...\n",
       "82           gs aapl amzn need to lead us higher spx dji\n",
       "84     active traders try one of these free trading g...\n",
       "85     xplr join us for play by play action on stocks...\n",
       "86     amzn pzza restaurants are in a tech race to ma...\n",
       "96     amzn part bmark offering guidance y y y y y y ...\n",
       "102    hot options alert midday tuesday august bac dk...\n",
       "108    there is a chance apple could be making a doub...\n",
       "113    join and well both get a share of stock like a...\n",
       "117    retail never learns when buying stock you dont...\n",
       "120    why investors should love the new tsla semitru...\n",
       "123    tsla has traded volume of yesterday and nearly...\n",
       "125    a collection of testimonials from satisfied su...\n",
       "127    why investors should love the new tsla semitru...\n",
       "133    dont be too proud to copy is fb internal motto...\n",
       "135    fb new facebook data center a boost to ohios t...\n",
       "138    amazing tandemtrader is amazing if you want to...\n",
       "140    boost your focus with these simple exercises y...\n",
       "170    the real reason ibm is like a utility csco d d...\n",
       "196    china lchaim make me a match ginni cant even d...\n",
       "219    quantum computing t revolutionize ai ml bigdat...\n",
       "222                           ibm a good opportunity ibm\n",
       "225    amazon baird likes hulu win expanding tool set...\n",
       "236    pretty basic set up aezs lets see what where s...\n",
       "241    ltea huge news out frsx cvm mrns tnk opht tops...\n",
       "249    digaf wake up people grab you a sure fire winn...\n",
       "251    aapl fb twtr snap googl tech companies urge su...\n",
       "259    digaf wake up people grab you a sure fire winn...\n",
       "264    opening lines id be looking at right now as po...\n",
       "299    top internet stocks on the market amzn ebay gr...\n",
       "301    ca h bandicoot available for ps and box buy no...\n",
       "307    first trust advisors lp raises stake in box in...\n",
       "330    j p marvel investment advisors llc has stake i...\n",
       "336    full free analysis posted on fang fb amzn nflx...\n",
       "339    free make more money with us deals cash dowjon...\n",
       "342    full free analysis posted on fang fb amzn nflx...\n",
       "345    community trust investment co raises stake in ...\n",
       "398    community trust investment co has stake in inc...\n",
       "409    amzn more powerful than you be careful who you...\n",
       "411    dailymarketpoll who will win the streaming war...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['positive'] >0) & (df['negative']  == 0)]['text'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25     goog neonazi group moves to dark web after web...\n",
       "43     will advances in ai ml robotics nanotech genom...\n",
       "47     hpc ai ml bigdata may soon enable genome editi...\n",
       "56     discussing the retail landscape department sto...\n",
       "67     sitrep risk on mrk ceo youre fired amzn gs leg...\n",
       "79     amzn aap wmt amazon will probably go onto crus...\n",
       "80     dont worry about how many shares you can buy c...\n",
       "95     amazon will probably go onto crush auto parts ...\n",
       "119    tsla sa another risk factor for tesla shorts d...\n",
       "145    microsoft acquires cloudcomputing orchestratio...\n",
       "185    international business machines ibm fall to no...\n",
       "187             the blue cloud collapses i told u ibm so\n",
       "191    so u wont ask ginni about her two ibm failed a...\n",
       "192    seekingalpha ibm watson disappointment risks f...\n",
       "193    ibm watson disappointment risks further downwa...\n",
       "200    china big market thus saith ginni so far zero ...\n",
       "205    is ibms dividend yield killing strategic imper...\n",
       "206    is ibms dividend yield killing strategic imper...\n",
       "232    snap cantor fitzgerald sees almost upside for ...\n",
       "233    snap cantor fitzgerald sees almost upside for ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['positive'] ==0) & (df['negative']  > 0)]['text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 8434\n",
      "Total tweets positive: 2219\n",
      "Total tweets negative: 1246\n",
      "Tweets with no info: 4320\n",
      "neutral tweets: 649\n"
     ]
    }
   ],
   "source": [
    "print \"Total tweets:\", len(df)\n",
    "print \"Total tweets positive:\",len(df[(df['positive'] >0) & (df['negative']  == 0)])\n",
    "print \"Total tweets negative:\",len(df[(df['positive'] == 0) & (df['negative']  > 0)])\n",
    "print \"Tweets with no info:\", len(df[(df['positive'] == 0) & (df['negative']  == 0)])\n",
    "print \"neutral tweets:\", len(df[(df['positive'] >0) & (df['negative']  > 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/Users/jenchieh/my_Github/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting each tweet into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "matrix = []\n",
    "\n",
    "df = df.reset_index().drop('index', axis=1)\n",
    "        \n",
    "for text in df.text:\n",
    "    filtered_text = [model[w] for w in text.split() if w in model]\n",
    "    if len(filtered_text):\n",
    "        matrix.append(matutils.unitvec(np.array(filtered_text).mean(axis=0)))\n",
    "    else:\n",
    "        matrix.append(np.zeros(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8434, 300)\n",
      "[[ 1.          0.6802576   0.63090704 ...,  0.3900484   0.55352346\n",
      "   0.12610915]\n",
      " [ 0.6802576   1.          0.64229834 ...,  0.32421411  0.46891389\n",
      "   0.14354429]\n",
      " [ 0.63090704  0.64229834  1.         ...,  0.32880703  0.47104502\n",
      "   0.24152685]\n",
      " ..., \n",
      " [ 0.3900484   0.32421411  0.32880703 ...,  1.          0.59902306\n",
      "   0.69015347]\n",
      " [ 0.55352346  0.46891389  0.47104502 ...,  0.59902306  1.          0.38061422]\n",
      " [ 0.12610915  0.14354429  0.24152685 ...,  0.69015347  0.38061422  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array(matrix)\n",
    "print matrix.shape\n",
    "sim = np.dot(matrix, matrix.transpose())\n",
    "print sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8434, 8434)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.556196e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.484151e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.223216e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.224153e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.672356e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.495172e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.318481e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  3.556196e+07\n",
       "mean   4.484151e-01\n",
       "std    1.223216e-01\n",
       "min   -1.224153e-01\n",
       "25%    3.672356e-01\n",
       "50%    4.495172e-01\n",
       "75%    5.318481e-01\n",
       "max    1.000000e+00"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print sim.shape\n",
    "np.fill_diagonal(sim, 0)\n",
    "simdf = pd.DataFrame(list(sim[np.triu_indices(sim.shape[1], 1)]))\n",
    "simdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.85482778284 2468\n",
      "with exascale expect quantum leap in ai ml robotics aerospace auto pharma goog msft intc aapl amd amzn f gm\n",
      "quantum computing t revolutionize ai ml bigdata genomics pharma biotech robotics msft goog ibm\n"
     ]
    }
   ],
   "source": [
    "#### Get the most similar tweets for each sentiment\n",
    "pos = 19\n",
    "most_similar = np.argmax(sim[pos][:])\n",
    "print \"similarity:\", sim[pos][most_similar], most_similar\n",
    "print re_df.iloc[pos].text\n",
    "print re_df.iloc[most_similar].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.814871472026 3628\n",
      "first trust advisors lp raises stake in box inc box\n",
      "federated investors inc pa has million stake in box inc box\n"
     ]
    }
   ],
   "source": [
    "#### Get the most similar tweets for each sentiment\n",
    "neg = 191\n",
    "most_similar = np.argmax(sim[neg][:])\n",
    "print \"similarity:\", sim[neg][most_similar], most_similar\n",
    "print re_df.iloc[neg].text\n",
    "print re_df.iloc[most_similar].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8434, 300), (8434, 12))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost Classifier\n",
    "\n",
    " - Train-Test Split\n",
    " - Grid Search on 3-fold CV\n",
    " - Perform predition by the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3465,), (3465, 300))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_idx = df.label != 0\n",
    "y = df.label[selected_idx]\n",
    "X = matrix[selected_idx]\n",
    "\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=30 .................................................\n",
      "[CV] .................................. n_estimators=30, total=   2.5s\n",
      "[CV] .................................. n_estimators=30, total=   2.5s\n",
      "[CV] .................................. n_estimators=30, total=   2.6s\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=300 ................................................\n",
      "[CV] ................................. n_estimators=100, total=   8.8s\n",
      "[CV] n_estimators=300 ................................................\n",
      "[CV] ................................. n_estimators=100, total=   8.9s\n",
      "[CV] n_estimators=300 ................................................\n",
      "[CV] ................................. n_estimators=100, total=   9.1s\n",
      "[CV] ................................. n_estimators=300, total=  23.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:   26.5s remaining:    7.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. n_estimators=300, total=  21.6s\n",
      "[CV] ................................. n_estimators=300, total=  20.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   32.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [30, 100, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={'n_estimators': [30, 100, 300]}\n",
    "\n",
    "adb_cv = GridSearchCV(AdaBoostClassifier(), \n",
    "                      param_grid=params, scoring=make_scorer(f1_score),\n",
    "                      cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "adb_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.852813852814\n",
      "F1 Score: 0.885650224215\n"
     ]
    }
   ],
   "source": [
    "estimator = adb_cv.best_estimator_\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "print 'Accuracy:', accuracy_score(y_test, y_pred)\n",
    "print 'F1 Score:', f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
